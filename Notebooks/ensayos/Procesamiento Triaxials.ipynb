{"cells":[{"cell_type":"code","source":["# ─── Celda 1: Definición de esquemas y funciones auxiliares ─────────────────────────────\n","from pyspark.sql import functions as F\n","from pyspark.sql.types import *\n","import uuid\n","import pandas as pd\n","from notebookutils import mssparkutils\n","import tempfile\n","import os\n","\n","# 1. Definición de esquemas\n","\n","sample_schema = StructType([\n","    StructField(\"uuid\", StringType(), False),\n","    StructField(\"SampleID\", StringType(), True),       # Será el valor de CODE\n","    StructField(\"LocationID\", StringType(), True),      # Siempre null por ahora\n","    StructField(\"SampleType\", StringType(), True),      # “Sample Type” en el Excel\n","    StructField(\"MaterialType\", StringType(), True),    # “Material group” en el Excel\n","    StructField(\"State\", StringType(), True),           # Siempre null\n","    StructField(\"DepthFrom_m\", DoubleType(), True),\n","    StructField(\"DepthTo_m\", DoubleType(), True),\n","    StructField(\"MiddleDepth_m\", DoubleType(), True),\n","    StructField(\"Laboratory\", StringType(), True),      # Siempre null\n","    StructField(\"Comment\", StringType(), True),         # Siempre null\n","    StructField(\"createdAt\", TimestampType(), False)\n","])\n","\n","labtest_schema = StructType([\n","    StructField(\"uuid\", StringType(), False),\n","    StructField(\"LabTestID\", StringType(), False),      # Valor de “Test”\n","    StructField(\"SampleID\", StringType(), False),       # Será el uuid generado para el sample\n","    StructField(\"TestType\", StringType(), True),        # Valor de “CD/CU”\n","    StructField(\"TestDate\", DateType(), True),          # Siempre null\n","    StructField(\"ReceivedDate\", DateType(), True),      # Siempre null\n","    StructField(\"Comment\", StringType(), True),         # Siempre null\n","    StructField(\"createdAt\", TimestampType(), False)\n","])\n","\n","triaxial_schema = StructType([\n","    StructField(\"uuid\", StringType(), False),\n","    StructField(\"LabTestID\", StringType(), False),\n","    StructField(\"TriaxialType\", StringType(), True),\n","    StructField(\"Direction\", StringType(), True),\n","    StructField(\"ConsolidationType\", StringType(), True),\n","    StructField(\"AxialStrain_Percent\", DoubleType(), True),\n","    StructField(\"VolumetricStrain_Percent\", DoubleType(), True),\n","    StructField(\"MajorPrincipalEffectiveStress_kPa\", DoubleType(), True),\n","    StructField(\"MinorPrincipalEffectiveStress_kPa\", DoubleType(), True),\n","    StructField(\"ExcessPorePressure_kPa\", DoubleType(), True),\n","    StructField(\"VoidRatio\", DoubleType(), True),\n","    StructField(\"createdAt\", TimestampType(), False)\n","])\n","\n","# 2. Función para generar UUID determinista\n","def generate_uuid(specimen):\n","    namespace_uuid = uuid.UUID('6ba7b810-9dad-11d1-80b4-00c04fd430c8')\n","    return str(uuid.uuid5(namespace_uuid, str(specimen)))\n","\n","# Registrar UDF para generar UUID en Spark\n","spark.udf.register(\"generate_uuid_udf\", generate_uuid, StringType())\n","\n","# 3. Función para cálculo de profundidades (struct con DepthFrom_m, DepthTo_m, MiddleDepth_m)\n","@F.udf(returnType=StructType([\n","    StructField(\"DepthFrom_m\", DoubleType()),\n","    StructField(\"DepthTo_m\", DoubleType()),\n","    StructField(\"MiddleDepth_m\", DoubleType())\n","]))\n","def calculate_depths(depth_str):\n","    if depth_str is None:\n","        return (None, None, None)\n","    try:\n","        depth_str = str(depth_str).lower().replace(\"m\", \"\").strip()\n","        if \"-\" in depth_str:\n","            parts = depth_str.split(\"-\")\n","            from_depth = float(parts[0].strip())\n","            to_depth = float(parts[1].strip())\n","            return (from_depth, to_depth, (from_depth + to_depth) / 2)\n","        else:\n","            depth = float(depth_str)\n","            return (depth, depth, depth)\n","    except:\n","        return (None, None, None)\n","\n","# 4. Función para cargar archivos Excel (usa mssparkutils y pandas)\n","def load_excel(file_path):\n","    \"\"\"Carga un archivo Excel desde Files usando mssparkutils y retorna un Spark DataFrame.\"\"\"\n","    temp_dir = tempfile.gettempdir()\n","    local_file = os.path.join(temp_dir, os.path.basename(file_path))\n","    mssparkutils.fs.cp(file_path, f\"file:{local_file}\", True)\n","    pdf = pd.read_excel(local_file)\n","    return spark.createDataFrame(pdf)\n","\n","# 5. Función para procesar datos triaxiales hoja por hoja (igual que antes)\n","def process_triaxial_sheet(row, excel_data_path):\n","    try:\n","        sheet_name = row[\"Sheetname\"]\n","        first_row = int(row[\"Fisrt row\"])   # p.ej. 2\n","        last_row = int(row[\"Last row\"])     # p.ej. 145\n","\n","        temp_dir = tempfile.gettempdir()\n","        local_file = os.path.join(temp_dir, os.path.basename(excel_data_path))\n","        mssparkutils.fs.cp(excel_data_path, f\"file:{local_file}\", True)\n","\n","        # Leer la hoja con pandas: skiprows=first_row-1, header=[0,1]\n","        df_sheet = pd.read_excel(\n","            local_file,\n","            sheet_name=sheet_name,\n","            skiprows=first_row - 1,\n","            header=[0, 1]\n","        )\n","        if df_sheet.empty:\n","            print(f\"⚠️ Advertencia: Hoja vacía - Test: {row['Test']}, Hoja: {sheet_name}\")\n","            return None\n","\n","        # Aplanar MultiIndex de columnas concatenando ambos niveles\n","        df_sheet.columns = [\n","            f\"{str(col[0]).strip()}_{str(col[1]).strip()}\"\n","            for col in df_sheet.columns.to_list()\n","        ]\n","\n","        # Calcular cuántas filas de datos reales hay: last_row - first_row - 1\n","        num_datos = last_row - first_row - 1\n","        df_sheet = df_sheet.iloc[:num_datos]\n","\n","        # Renombrar columnas de pandas al esquema triaxial_schema\n","        column_mapping = {\n","            \"Axial Strain (%)_\":              \"AxialStrain_Percent\",\n","            \"Volumetric Strain (%)_\":         \"VolumetricStrain_Percent\",\n","            \"Major Principal Stress (kPa)_\":  \"MajorPrincipalEffectiveStress_kPa\",\n","            \"Minor Principal Stress (kPa)_\":  \"MinorPrincipalEffectiveStress_kPa\",\n","            \"Pore Pressure (kPa)_\":           \"ExcessPorePressure_kPa\",\n","            \"Void Ratio_\":                     \"VoidRatio\"\n","        }\n","        df_sheet.rename(columns=column_mapping, inplace=True)\n","\n","        # Convertir a float los campos DoubleType para evitar errores con ints\n","        double_fields = [\n","            \"AxialStrain_Percent\",\n","            \"VolumetricStrain_Percent\",\n","            \"MajorPrincipalEffectiveStress_kPa\",\n","            \"MinorPrincipalEffectiveStress_kPa\",\n","            \"ExcessPorePressure_kPa\",\n","            \"VoidRatio\"\n","        ]\n","        for fld in double_fields:\n","            if fld in df_sheet.columns:\n","                df_sheet[fld] = df_sheet[fld].apply(lambda x: float(x) if pd.notnull(x) else None)\n","\n","        # Añadir columnas obligatorias: LabTestID, TriaxialType, uuid, createdAt\n","        df_sheet[\"LabTestID\"]    = row[\"Test\"]\n","        df_sheet[\"TriaxialType\"] = row[\"CD/CU\"]\n","        df_sheet[\"uuid\"]         = [str(uuid.uuid4()) for _ in range(len(df_sheet))]\n","        df_sheet[\"createdAt\"]    = pd.Timestamp.now()\n","\n","        # Rellenar cualquier columna del esquema que falte\n","        for col in triaxial_schema.fieldNames():\n","            if col not in df_sheet.columns:\n","                df_sheet[col] = None\n","\n","        # Seleccionar únicamente las columnas definidas en el esquema\n","        df_sheet = df_sheet[triaxial_schema.fieldNames()]\n","\n","        # Convertir NaN de pandas a None para evitar errores en Spark\n","        df_sheet = df_sheet.where(pd.notnull(df_sheet), None)\n","\n","        return spark.createDataFrame(df_sheet, schema=triaxial_schema)\n","\n","    except Exception as e:\n","        print(f\"❌ Error procesando {row['Test']}: {str(e)}\")\n","        return None\n","\n","# Rutas de archivos en Files/Triaxials\n","SAMPLES_PATH = \"Files/Triaxials/Sampleslist using lab numbers .xlsx\"\n","DATA_PATH    = \"Files/Triaxials/SRK-169-TX-ExcelData.xlsx\"\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":18,"statement_ids":[18],"state":"finished","livy_statement_state":"available","session_id":"8465ea11-cd89-4d55-a3ff-2691f60a7fde","normalized_state":"finished","queued_time":"2025-06-04T22:40:41.836766Z","session_start_time":null,"execution_start_time":"2025-06-04T22:40:41.8382716Z","execution_finish_time":"2025-06-04T22:40:42.2244805Z","parent_msg_id":"3823e1a4-e108-4e1a-ae3a-e40d7a0d0138"},"text/plain":"StatementMeta(, 8465ea11-cd89-4d55-a3ff-2691f60a7fde, 18, Finished, Available, Finished)"},"metadata":{}}],"execution_count":17,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2167413d-e904-4eb1-90fd-551382a93448"},{"cell_type":"code","source":["# ─── Celda 2 (CORREGIDA): Carga y procesamiento de datos de Sample y LabTest ───────────\n","\n","# 1. Cargar lista de muestras desde el Excel de Sample list\n","samples_df = load_excel(SAMPLES_PATH)\n","\n","# 2. Generar la tabla de samples a partir de cada CODE único\n","\n","# a) Tomar sólo un registro por “Code” (cada código de muestra debe aparecer una vez)\n","distinct_samples = samples_df.dropDuplicates([\"Code\"]) \\\n","    .select(\n","        F.col(\"Code\").alias(\"SampleCode\"),\n","        F.col(\"Sample Type\"),\n","        F.col(\"Material Group\"),\n","        F.col(\"Depth (m)\")\n","    )\n","\n","# b) Añadir uuid y createdAt, luego calcular depths y asignar columnas fijas\n","sample_table = distinct_samples \\\n","    .withColumn(\"uuid\", F.expr(\"generate_uuid_udf(SampleCode)\")) \\\n","    .withColumn(\"createdAt\", F.current_timestamp()) \\\n","    .withColumn(\"DepthInfo\", calculate_depths(F.col(\"Depth (m)\"))) \\\n","    .select(\n","        F.col(\"uuid\"),\n","        F.col(\"SampleCode\").cast(\"string\").alias(\"SampleID\"),\n","        F.lit(None).cast(\"string\").alias(\"LocationID\"),\n","        F.col(\"Sample Type\").alias(\"SampleType\"),\n","        F.col(\"Material Group\").alias(\"MaterialType\"),\n","        F.lit(None).cast(\"string\").alias(\"State\"),\n","        F.col(\"DepthInfo.DepthFrom_m\").alias(\"DepthFrom_m\"),\n","        F.col(\"DepthInfo.DepthTo_m\").alias(\"DepthTo_m\"),\n","        F.col(\"DepthInfo.MiddleDepth_m\").alias(\"MiddleDepth_m\"),\n","        F.lit(None).cast(\"string\").alias(\"Laboratory\"),\n","        F.lit(None).cast(\"string\").alias(\"Comment\"),\n","        F.col(\"createdAt\")\n","    )\n","\n","# c) Crear un mapping CODE → sample_uuid para usar en labtest\n","sample_map = sample_table.select(\"SampleID\", \"uuid\").withColumnRenamed(\"uuid\", \"SampleUUID\")\n","\n","# 3. Generar la tabla de labtests (una fila por cada “Test” único)\n","\n","# a) Seleccionar sólo un registro por Test (para evitar duplicados)\n","distinct_labtests = samples_df.dropDuplicates([\"Test\"]) \\\n","    .select(\n","        F.col(\"Code\").alias(\"SampleCode\"),\n","        F.col(\"Test\").alias(\"LabTestID\"),\n","        F.col(\"CD/CU\").alias(\"TestType\")\n","    )\n","\n","# b) Unir con sample_map para obtener el SampleUUID y generar uuid propio\n","labtest_table = distinct_labtests.join(\n","    sample_map,\n","    distinct_labtests[\"SampleCode\"] == sample_map[\"SampleID\"],\n","    how=\"left\"\n",").select(\n","    F.expr(\"generate_uuid_udf(LabTestID)\").alias(\"uuid\"),\n","    F.col(\"LabTestID\"),\n","    F.col(\"SampleUUID\").alias(\"SampleID\"),\n","    F.col(\"TestType\"),\n","    F.lit(None).cast(\"date\").alias(\"TestDate\"),\n","    F.lit(None).cast(\"date\").alias(\"ReceivedDate\"),\n","    F.lit(None).cast(\"string\").alias(\"Comment\"),\n","    F.current_timestamp().alias(\"createdAt\")\n",")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":19,"statement_ids":[19],"state":"finished","livy_statement_state":"available","session_id":"8465ea11-cd89-4d55-a3ff-2691f60a7fde","normalized_state":"finished","queued_time":"2025-06-04T22:40:42.238149Z","session_start_time":null,"execution_start_time":"2025-06-04T22:40:42.2392581Z","execution_finish_time":"2025-06-04T22:40:43.1249394Z","parent_msg_id":"6c43997a-f3c7-4ffc-96da-7188119437bd"},"text/plain":"StatementMeta(, 8465ea11-cd89-4d55-a3ff-2691f60a7fde, 19, Finished, Available, Finished)"},"metadata":{}}],"execution_count":18,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a977e114-db89-4744-8d15-5c9b15434711"},{"cell_type":"code","source":["# ─── Celda 3: Escritura en Lakehouse Silver y procesamiento de Triaxial ────────────────\n","# Propiedades Delta para optimización y CDC\n","delta_properties = {\n","    \"delta.enableChangeDataFeed\": \"true\",\n","    \"delta.autoOptimize.autoCompact\": \"true\"\n","}\n","\n","# 4. Escribir tabla Sample en silver.dbo.Sample\n","sample_table.write.format(\"delta\") \\\n","    .mode(\"append\") \\\n","    .option(\"mergeSchema\", \"true\") \\\n","    .options(**delta_properties) \\\n","    .saveAsTable(\"silver.dbo.Sample\")\n","\n","# 5. Escribir tabla LabTest en silver.dbo.LabTest\n","labtest_table.write.format(\"delta\") \\\n","    .mode(\"append\") \\\n","    .option(\"mergeSchema\", \"true\") \\\n","    .options(**delta_properties) \\\n","    .saveAsTable(\"silver.dbo.LabTest\")\n","\n","# 6. Procesar y escribir datos triaxiales (capa Triaxial)\n","triaxial_dfs = []\n","for row in samples_df.filter(F.col(\"CD/CU\").isin([\"CU\", \"CD\"])).collect():\n","    df_triax = process_triaxial_sheet(row, DATA_PATH)\n","    if df_triax:\n","        triaxial_dfs.append(df_triax)\n","        print(f\"✅ Triaxial procesado: {row['Test']}\")\n","\n","if triaxial_dfs:\n","    triaxial_table = triaxial_dfs[0]\n","    for df_next in triaxial_dfs[1:]:\n","        triaxial_table = triaxial_table.unionByName(df_next)\n","\n","    # 7. Escribir tabla Triaxial en silver.dbo.Triaxial\n","    triaxial_table.write.format(\"delta\") \\\n","        .mode(\"append\") \\\n","        .option(\"mergeSchema\", \"true\") \\\n","        .options(**delta_properties) \\\n","        .saveAsTable(\"silver.dbo.Triaxial\")\n","else:\n","    print(\"⚠️ No se procesaron datos triaxiales\")\n","\n","# 8. Validación y reporte final\n","print(\"\\n\" + \"=\"*50)\n","print(\"RESUMEN DE CARGA EN SILVER\")\n","print(\"=\"*50)\n","\n","sample_count   = spark.sql(\"SELECT COUNT(*) FROM silver.dbo.Sample\").first()[0]\n","labtest_count  = spark.sql(\"SELECT COUNT(*) FROM silver.dbo.LabTest\").first()[0]\n","triaxial_count = 0\n","try:\n","    triaxial_count = spark.sql(\"SELECT COUNT(*) FROM silver.dbo.Triaxial\").first()[0]\n","except:\n","    pass\n","\n","print(f\"Muestras (Sample):                {sample_count} registros\")\n","print(f\"Ensayos (LabTest):               {labtest_count} registros\")\n","print(f\"Datos triaxiales (Triaxial):     {triaxial_count} registros\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":20,"statement_ids":[20],"state":"finished","livy_statement_state":"available","session_id":"8465ea11-cd89-4d55-a3ff-2691f60a7fde","normalized_state":"finished","queued_time":"2025-06-04T22:40:42.6179009Z","session_start_time":null,"execution_start_time":"2025-06-04T22:40:43.1270629Z","execution_finish_time":"2025-06-04T22:41:15.3927773Z","parent_msg_id":"fa5b7975-19e4-4255-8596-331880ba8aec"},"text/plain":"StatementMeta(, 8465ea11-cd89-4d55-a3ff-2691f60a7fde, 20, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["✅ Triaxial procesado: 3095-T1-CU\n✅ Triaxial procesado: 3095-T2-CU\n✅ Triaxial procesado: 3095-T3-CU\n✅ Triaxial procesado: 3095-T1-CD\n✅ Triaxial procesado: 3095-T2-CD\n✅ Triaxial procesado: 3095-T3-CD\n✅ Triaxial procesado: 3096-T1-CU\n✅ Triaxial procesado: 3096-T2-CU\n✅ Triaxial procesado: 3096-T3-CU\n✅ Triaxial procesado: 3096-T1-CD\n✅ Triaxial procesado: 3096-T2-CD\n✅ Triaxial procesado: 3096-T3-CD\n✅ Triaxial procesado: 3099-T1-CU\n✅ Triaxial procesado: 3099-T2-CU\n✅ Triaxial procesado: 3099-T3-CU\n✅ Triaxial procesado: 3099-T1-CD\n✅ Triaxial procesado: 3099-T2-CD\n✅ Triaxial procesado: 3099-T3-CD\n✅ Triaxial procesado: 3112-T1-CU\n✅ Triaxial procesado: 3112-T2-CU\n✅ Triaxial procesado: 3112-T3-CU\n✅ Triaxial procesado: 3113-T1-CU\n✅ Triaxial procesado: 3113-T3-CU\n✅ Triaxial procesado: 3114-T1-CU\n✅ Triaxial procesado: 3114-T2-CU\n✅ Triaxial procesado: 3114-T3-CU\n✅ Triaxial procesado: 3109-T1-CU\n✅ Triaxial procesado: 3109-T2-CU\n✅ Triaxial procesado: 3109-T3-CU\n✅ Triaxial procesado: 3110-T1-CU\n✅ Triaxial procesado: 3110-T2-CU\n✅ Triaxial procesado: 3110-T3-CU\n✅ Triaxial procesado: 3110-T1-CD\n✅ Triaxial procesado: 3110-T2-CD\n✅ Triaxial procesado: 3110-T3-CD\n✅ Triaxial procesado: 3101-T1-CU\n✅ Triaxial procesado: 3101-T2-CU\n✅ Triaxial procesado: 3101-T3-CU\n✅ Triaxial procesado: 3101-T1-CD\n✅ Triaxial procesado: 3101-T2-CD\n✅ Triaxial procesado: 3101-T3-CD\n✅ Triaxial procesado: 3107-T1-CU\n✅ Triaxial procesado: 3107-T2-CU\n✅ Triaxial procesado: 3107-T3-CU\n✅ Triaxial procesado: 3108-T1-CU\n✅ Triaxial procesado: 3108-T2-CU\n✅ Triaxial procesado: 3108-T3-CU\n\n==================================================\nRESUMEN DE CARGA EN SILVER\n==================================================\nMuestras (Sample):                11 registros\nEnsayos (LabTest):               47 registros\nDatos triaxiales (Triaxial):     6716 registros\n"]}],"execution_count":19,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"ce8e7679-3668-426c-8793-29753c472ad8"},{"cell_type":"code","source":["%%sql\n","SELECT * FROM atterberg"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":28,"statement_ids":[28],"state":"finished","livy_statement_state":"available","session_id":"8465ea11-cd89-4d55-a3ff-2691f60a7fde","normalized_state":"finished","queued_time":"2025-06-04T22:50:24.1483523Z","session_start_time":null,"execution_start_time":"2025-06-04T22:50:24.149534Z","execution_finish_time":"2025-06-04T22:50:27.0261834Z","parent_msg_id":"c0a6f2e7-2487-4030-a451-b2e4df5887b9"},"text/plain":"StatementMeta(, 8465ea11-cd89-4d55-a3ff-2691f60a7fde, 28, Finished, Available, Finished)"},"metadata":{}},{"output_type":"execute_result","execution_count":27,"data":{"application/vnd.synapse.sparksql-result+json":{"schema":{"type":"struct","fields":[{"name":"uuid","type":"string","nullable":false,"metadata":{}},{"name":"LabTestID","type":"string","nullable":false,"metadata":{}},{"name":"LiquidLimit","type":"double","nullable":true,"metadata":{}},{"name":"PlasticLimit","type":"double","nullable":true,"metadata":{}},{"name":"PlasticityIndex","type":"double","nullable":true,"metadata":{}},{"name":"Classification","type":"string","nullable":true,"metadata":{}},{"name":"createdAt","type":"timestamp","nullable":false,"metadata":{}}]},"data":[["09e6ec20-20e7-4bdd-a47e-050483126945","SRK-169-3098",null,null,0,"","2025-05-31T00:49:02Z"],["9976a13b-30ad-41df-a300-df95b9923518","SRK-169-3099",null,null,0,"","2025-05-31T00:49:02Z"],["d165e80c-97b3-4802-8696-56efde7264f3","SRK-169-3094",null,null,0,"","2025-05-31T00:49:02Z"],["19256532-3d7a-46bd-9ed6-78cd780d89bc","SRK-169-3095",null,null,0,"","2025-05-31T00:49:02Z"],["406dd9d8-09e9-4eb1-ae7d-adaf8854dbe7","SRK-169-3105",44,16,0,"","2025-05-31T00:49:02Z"],["0b3b2407-52e6-4dd9-bd1d-2f6b963f31fa","SRK-169-3110",40,15,0,"","2025-05-31T00:49:02Z"],["0bda9d36-3478-4369-aa56-3a95e7b6aa66","SRK-169-3096",null,null,0,"","2025-05-31T00:49:02Z"],["3d4bd77e-8f0d-4630-8a63-c15de43314ae","SRK-169-3097",null,null,0,"","2025-05-31T00:49:02Z"],["6f2899bc-79f9-44a9-b458-7bf69781d3d7","SRK-169-3104",43,16,0,"","2025-05-31T00:49:02Z"],["a7367878-8a5c-4c0f-b451-71b10b1bb2e0","SRK-169-3111",29,3,0,"","2025-05-31T00:49:02Z"],["c193aead-eecf-428f-a2d1-0722888abf6d","SRK-169-3112",null,null,0,"","2025-05-31T00:49:02Z"],["33b5f5f2-5306-4bd7-9c59-d5fd168b79ed","SRK-169-3113",null,null,0,"","2025-05-31T00:49:02Z"],["7ed3da08-0e68-463b-bbd3-56fe866c6c9c","SRK-169-3114",null,null,0,"","2025-05-31T00:49:02Z"],["23591e32-12f3-4ec0-904c-e75b31382e59","SRK-169-3102",null,null,0,"","2025-05-31T00:49:02Z"],["e9f3cef4-7058-4eb1-9734-ace5b7662667","SRK-169-3103",41,13,0,"","2025-05-31T00:49:02Z"],["d6cca85a-30fe-438a-8849-456ba37016c6","SRK-169-3100",null,null,0,"","2025-05-31T00:49:02Z"],["3dbf6210-c21d-4cb3-b515-7cf21a4f17df","SRK-169-3101",null,null,0,"","2025-05-31T00:49:02Z"],["8ede975d-9502-4d8d-aa92-e45de482cec3","SRK-169-3103",41,13,0,"","2025-05-31T00:49:02Z"],["4c89794f-df8c-49b2-ba49-72ee69f72bc1","SRK-169-3104",39,13,0,"","2025-05-31T00:49:02Z"],["2dd34d21-8ddd-4110-a0f7-da987fe7cd6e","SRK-169-3105",44,15,0,"","2025-05-31T00:49:02Z"],["275449a9-7769-4583-b716-647fc488051e","SRK-169-3109",null,null,0,"","2025-05-31T00:49:02Z"]]},"text/plain":"<Spark SQL result set with 21 rows and 7 fields>"},"metadata":{}}],"execution_count":27,"metadata":{"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"collapsed":false},"id":"ef16b7db-a37d-4818-8505-685401b45a93"},{"cell_type":"code","source":[],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"af1a0e34-941e-4f08-9f39-43134674d0c7"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"c14a86b2-f247-44a5-8d06-0dafd853babf"}],"default_lakehouse":"c14a86b2-f247-44a5-8d06-0dafd853babf","default_lakehouse_name":"silver","default_lakehouse_workspace_id":"cd0fc9d8-dd24-4361-bd63-c66c06cf2316"}}},"nbformat":4,"nbformat_minor":5}