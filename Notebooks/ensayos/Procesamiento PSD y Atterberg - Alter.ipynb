{"cells":[{"cell_type":"code","source":["# Tablas para completar con PSD y Atterberg\n","\n","# Project, Program y Location deben ser cargados casi completamente manualmente para PSD\n","\n","# Location -> Podemos tener el TP-1 pero eso no es global y deberia ser usado\n","# en conjunto con programa y/o proyecto para ser utilizados como ID\n","\n","# Sample -> Como ID podemos tener un UUID o utilizar el Sample ID que viene en el pdf\n","# Otra vez, el valor del PDF no el global por lo que deberiamos utilizar un ID compuesto\n","# Por otro lado el ensayo anterior utiliza UUIDs para eso, lo que me parece mejor\n","# From y To salen de los datos\n","\n","# LabTest -> LabTestID (SRK-123-1234), el resto de los datos no estan en el informe\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":60,"statement_ids":[60],"state":"finished","livy_statement_state":"available","session_id":"73e84513-303a-44b1-8e97-abaecbeeff4d","normalized_state":"finished","queued_time":"2025-05-19T04:52:36.229915Z","session_start_time":null,"execution_start_time":"2025-05-19T04:52:36.2311149Z","execution_finish_time":"2025-05-19T04:52:36.5685316Z","parent_msg_id":"cd7fa5fa-0ada-4e65-a782-a7c7a583c4a3"},"text/plain":"StatementMeta(, 73e84513-303a-44b1-8e97-abaecbeeff4d, 60, Finished, Available, Finished)"},"metadata":{}}],"execution_count":58,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8368fb26-2fb2-4ada-a57c-296baea03e76"},{"cell_type":"code","source":["##\n","# Construye las tablas si es que no existen\n","##\n","from pyspark.sql.types import StructType, StructField, StringType, FloatType\n","\n","# PSD\n","psd_schema = StructType([\n","    StructField(\"LabTestID\", StringType(), nullable=False),\n","    StructField(\"PSDType\", StringType(), nullable=True),\n","    StructField(\"SieveSize_mm\", StringType(), nullable=False),\n","    StructField(\"PercentPassing\", StringType(), nullable=False),\n","])\n","\n","# Atterberg\n","atterberg_schema = StructType([\n","    StructField(\"LabTestID\", StringType(), nullable=False),\n","    StructField(\"LiquidLimit\", StringType(), nullable=True),\n","    StructField(\"PlasticLimit\", StringType(), nullable=True),\n","    StructField(\"PlasticiyIndex\", StringType(), nullable=True),\n","    StructField(\"ContractionPerc\", StringType(), nullable=True),\n","    StructField(\"Clasification\", StringType(), nullable=True),\n","])\n","\n","# GrainSize\n","grain_size_schema = StructType([\n","    StructField(\"LabTestID\", StringType(), nullable=False),\n","    StructField(\"CobblePercent\", FloatType(), nullable=True),\n","    StructField(\"GravelPercent\", FloatType(), nullable=True),\n","    StructField(\"SandPercent\", FloatType(), nullable=True),\n","    StructField(\"FinePercent\", FloatType(), nullable=True),\n","    StructField(\"SiltPercent\", FloatType(), nullable=True),\n","    StructField(\"ClayPercent\", FloatType(), nullable=True),\n","])\n","\n","# Crea las tablas si es que no existen ya\n","spark.createDataFrame([], psd_schema).write.format(\"delta\").mode(\"ignore\").saveAsTable(\"PSD\")\n","spark.createDataFrame([], atterberg_schema).write.format(\"delta\").mode(\"ignore\").saveAsTable(\"Atterberg\")\n","spark.createDataFrame([], grain_size_schema).write.format(\"delta\").mode(\"ignore\").saveAsTable(\"GrainSize\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":61,"statement_ids":[61],"state":"finished","livy_statement_state":"available","session_id":"73e84513-303a-44b1-8e97-abaecbeeff4d","normalized_state":"finished","queued_time":"2025-05-19T04:52:36.4826628Z","session_start_time":null,"execution_start_time":"2025-05-19T04:52:36.5704776Z","execution_finish_time":"2025-05-19T04:52:39.0193107Z","parent_msg_id":"97a5dba0-b713-4e64-b660-3c19d4366823"},"text/plain":"StatementMeta(, 73e84513-303a-44b1-8e97-abaecbeeff4d, 61, Finished, Available, Finished)"},"metadata":{}}],"execution_count":59,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4e8ea2b0-5aad-4580-952c-6bfd000b7dc0"},{"cell_type":"code","source":["##\n","# Leemos los datos que se van a procesar\n","##\n","import pandas as pd\n","from uuid import uuid4\n","\n","# Se leen desde la tabla de bronce todos los datos que corresponden a esta fuente de datos\n","PSD_SOURCE_NAME = \"SRK-169-Summary Sheet.pdf\"\n","\n","psd_df = spark.read.table(\"bronze_shortcut.data_PSD\").where(f\"Data_filename = '{PSD_SOURCE_NAME}'\").toPandas()\n","atter_df = spark.read.table(\"bronze_shortcut.data_atterberg\").where(f\"Data_filename = '{PSD_SOURCE_NAME}'\").toPandas()\n","\n","\n","def build_psd_records(full_psd, row_i):\n","    \"\"\"Toma una de las filas de la tabla de bronce y la convierte \n","    en un conjunto de valores de la tabla de plata\"\"\"\n","    # Solo queremos los valores\n","    sieve_sizes = list(set(full_psd.columns) - set(['Data_filename', 'Depth_m', 'GM', 'Lab_No', 'Sample']))\n","    df = full_psd.loc[:, sieve_sizes].iloc[row_i]\n","\n","    # Transponemos y movemos el indice a una columna\n","    df = df.T.reset_index()\n","\n","    # Renombramos las columnas para que matcheen con la tabla de PSD\n","    df = df.rename(columns={\"index\": \"SieveSize_mm\", row_i: \"PercentPassing\"})\n","\n","    # Agregamos los valores que faltan\n","    df[\"LabTestID\"] = full_psd.iloc[row_i][\"Lab_No\"]\n","    df[\"PSDType\"] = \"\"\n","\n","    return df\n","\n","\n","def build_PSD_table(df):\n","    \"\"\"Convierte todos los datos de PSD en registros de la tabla de plata\"\"\"\n","    all_psd = [\n","        build_psd_records(df, i)\n","        for i in range(len(df))\n","    ]\n","    \n","    return pd.concat(all_psd, ignore_index=True)\n","\n","\n","def build_atterberg_table(df):\n","    \"\"\"Convierte todos los datos de Atterberg en registros de la tabla de plata\"\"\"\n","    # Solo las columnas que nos interesan\n","    att = df.loc[:, [\"Lab_No\", \"LL_%\", \"PI_%\", \"LS_%\"]]\n","\n","    # Son renombradas\n","    att = att.rename(columns={\n","        \"Lab_No\": \"LabTestID\", \n","        \"LL_%\": \"LiquidLimit\", \n","        \"PI_%\": \"PlasticLimit\", \n","        \"LS_%\": \"ContractionPerc\"\n","    })\n","\n","    # Agregamos las que faltan\n","    att[\"PlasticiyIndex\"] = 0\n","    att[\"Clasification\"] = \"\"\n","    \n","    return att\n","\n","\n","def _calc_depths(depths):\n","    \"\"\"Calcula las profundidades \"From\", \"Middle\" y \"To\", las devuelve en tuplas\"\"\"\n","\n","    # Si solo tengo una profundidad; son iguales\n","    if \"-\" not in depths:\n","        return depths.strip(), depths.strip(), depths.strip()\n","    \n","    # Tengo dos profundidades, calculo la media\n","    profundidades = [float(d.strip()) for d in depths.split(\"-\")]\n","    prof_media = sum(profundidades)/2\n","\n","    return str(profundidades[0]), str(prof_media), str(profundidades[1])\n","\n","\n","def build_tables(df):\n","    \"\"\"Construye las tablas \"LabTest\" y \"Sample\" para la capa de plata\"\"\"\n","    # Solo nos insteresan ciertas columnas\n","    df = df.loc[:, [\"Lab_No\", \"Sample\", \"Depth_m\"]].rename(columns={\"Lab_No\": \"LabTestID\", \"Sample\": \"Comment\"})\n","\n","    # Calculamos los valores que nos faltan\n","    df[\"SampleID\"] = df.apply(lambda r: uuid4().hex, axis=1)\n","    df[\"DepthFrom_m\"] = df.apply(lambda r: _calc_depths(r[\"Depth_m\"])[0], axis=1)\n","    df[\"MiddleDepth_m\"] = df.apply(lambda r: _calc_depths(r[\"Depth_m\"])[1], axis=1)\n","    df[\"DepthTo_m\"] = df.apply(lambda r: _calc_depths(r[\"Depth_m\"])[2], axis=1)\n","\n","    # Some empty but mandatory columns\n","    df[\"TestType\"] = \"\"\n","    df[\"TestDate\"] = None\n","    df[\"ReceivedDate\"] = None\n","    df[\"LocationID\"] = None\n","    df[\"SampleType\"] = None\n","    df[\"MaterialType\"] = None\n","    df[\"State\"] = None\n","    df[\"Laboratory\"] = None\n","\n","    labtest = df.loc[:, [\n","        \"LabTestID\", \"SampleID\", \"TestType\", \"Comment\", \n","        \"TestDate\", \"ReceivedDate\"\n","    ]]\n","    sample =  df.loc[:, [\n","        \"SampleID\", \"DepthFrom_m\", \"MiddleDepth_m\", \"DepthTo_m\", \n","        \"Comment\", \"LocationID\", \"SampleType\", \"MaterialType\", \n","        \"State\", \"Laboratory\"\n","    ]]\n","\n","    return labtest, sample\n","\n","\n","PSD_table_data = build_PSD_table(psd_df)\n","Atterberg_table_data = build_atterberg_table(atter_df)\n","\n","labtest_table, sample_table = build_tables(psd_df)\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":62,"statement_ids":[62],"state":"finished","livy_statement_state":"available","session_id":"73e84513-303a-44b1-8e97-abaecbeeff4d","normalized_state":"finished","queued_time":"2025-05-19T04:52:36.6235784Z","session_start_time":null,"execution_start_time":"2025-05-19T04:52:39.0212126Z","execution_finish_time":"2025-05-19T04:52:40.5019799Z","parent_msg_id":"17855baa-2621-4dea-9726-1246797f1c99"},"text/plain":"StatementMeta(, 73e84513-303a-44b1-8e97-abaecbeeff4d, 62, Finished, Available, Finished)"},"metadata":{}}],"execution_count":60,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"496fbf86-8882-4922-a25c-f6ebdb028fab"},{"cell_type":"code","source":["##\n","# Guardamos los datos en las tablas de plata\n","##\n","\n","from delta.tables import DeltaTable\n","\n","\n","def guardar_datos(datos, nombre_tabla):\n","    \"\"\"Guarga los datos en la tabla dada.\n","    Los datos duplicados no se guardan\"\"\"\n","\n","    # Los datos son transformados a un Spark Dataframe\n","    nuevos_datos = spark.createDataFrame(datos)\n","\n","    # Cuando los campos NO coinciden se insertan los datos\n","    DeltaTable.forName(spark, nombre_tabla).alias(\"old\").merge(\n","        nuevos_datos.alias(\"new\"),\n","        \"old.LabTestID = new.LabTestID\"\n","    ).whenNotMatchedInsertAll().execute()\n","\n","\n","def guardar_datos_sample(datos):\n","    \"\"\"Guarga los datos en la tabla \"Sample\".\n","    Los datos duplicados no se guardan\"\"\"\n","\n","    # Los datos son transformados a un Spark Dataframe\n","    nuevos_datos = spark.createDataFrame(datos)\n","\n","    # Cuando los campos NO coinciden se insertan los datos\n","    DeltaTable.forName(spark, \"Sample\").alias(\"old\").merge(\n","        nuevos_datos.alias(\"new\"),\n","        \"old.SampleID = new.SampleID\"\n","    ).whenNotMatchedInsertAll().execute()\n","\n","\n","guardar_datos(PSD_table_data, \"PSD\")\n","guardar_datos(Atterberg_table_data, \"Atterberg\")\n","guardar_datos(labtest_table, \"LabTest\")\n","\n","\n","# La tabla de Sample tiene logica propia\n","guardar_datos_sample(sample_table)\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":63,"statement_ids":[63],"state":"finished","livy_statement_state":"available","session_id":"73e84513-303a-44b1-8e97-abaecbeeff4d","normalized_state":"finished","queued_time":"2025-05-19T04:52:37.0732832Z","session_start_time":null,"execution_start_time":"2025-05-19T04:52:40.5104586Z","execution_finish_time":"2025-05-19T04:52:45.4287896Z","parent_msg_id":"ee4a12f4-50b2-4458-a360-3d2844be94c9"},"text/plain":"StatementMeta(, 73e84513-303a-44b1-8e97-abaecbeeff4d, 63, Finished, Available, Finished)"},"metadata":{}}],"execution_count":61,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a996ae45-26f0-42b3-b3a1-0d119d63686a"},{"cell_type":"code","source":["%%sql\n","select * from sample limit 5"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":64,"statement_ids":[64],"state":"finished","livy_statement_state":"available","session_id":"73e84513-303a-44b1-8e97-abaecbeeff4d","normalized_state":"finished","queued_time":"2025-05-19T04:52:37.7175679Z","session_start_time":null,"execution_start_time":"2025-05-19T04:52:45.4310292Z","execution_finish_time":"2025-05-19T04:52:47.8382602Z","parent_msg_id":"f1a5796b-51d1-41e7-a6ac-1aa0cce2a6fa"},"text/plain":"StatementMeta(, 73e84513-303a-44b1-8e97-abaecbeeff4d, 64, Finished, Available, Finished)"},"metadata":{}},{"output_type":"execute_result","execution_count":62,"data":{"application/vnd.synapse.sparksql-result+json":{"schema":{"type":"struct","fields":[{"name":"SampleID","type":"string","nullable":true,"metadata":{}},{"name":"LocationID","type":"string","nullable":true,"metadata":{}},{"name":"SampleType","type":"string","nullable":true,"metadata":{}},{"name":"MaterialType","type":"string","nullable":true,"metadata":{}},{"name":"State","type":"string","nullable":true,"metadata":{}},{"name":"MiddleDepth_m","type":"double","nullable":true,"metadata":{}},{"name":"DepthFrom_m","type":"double","nullable":true,"metadata":{}},{"name":"DepthTo_m","type":"double","nullable":true,"metadata":{}},{"name":"Laboratory","type":"string","nullable":true,"metadata":{}},{"name":"Comment","type":"string","nullable":true,"metadata":{}}]},"data":[["6d0963bec5204f56be4c850270014a34",null,null,null,null,0.25,0,0.5,null,"TP1 (Sample\rID. 1)"],["13ff784631304c72a32511d755c0cd2a",null,null,null,null,0.25,0,0.5,null,"TP2 (Sample\rID. 3)"],["539fcdeda49f466481073cc3f02c5cef",null,null,null,null,1.25,0,2.5,null,"TP3 (Sample\rID. 22)"],["4772e994db054371a8ba0b2f6bb733a6",null,null,null,null,1.25,0,2.5,null,"TP1 (Sample\rID. 28)"],["2d57d30e7d3344a5880e098caa263601",null,null,null,null,1.25,0,2.5,null,"TP3 (Sample\rID. 30)"]]},"text/plain":"<Spark SQL result set with 5 rows and 10 fields>"},"metadata":{}}],"execution_count":62,"metadata":{"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"collapsed":false},"id":"682ece90-b298-4005-bb3c-a0a71ff8766a"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"57d1ca48-d9cb-4b64-9e08-be28725753a4"}],"default_lakehouse":"57d1ca48-d9cb-4b64-9e08-be28725753a4","default_lakehouse_name":"Silver_processed","default_lakehouse_workspace_id":"cd0fc9d8-dd24-4361-bd63-c66c06cf2316"}}},"nbformat":4,"nbformat_minor":5}