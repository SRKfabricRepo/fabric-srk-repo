{"cells":[{"cell_type":"code","source":["# Tablas para completar con PSD y Atterberg\n","\n","# Project, Program y Location deben ser cargados casi completamente manualmente para PSD\n","\n","# Location -> Podemos tener el TP-1 pero eso no es global y deberia ser usado\n","# en conjunto con programa y/o proyecto para ser utilizados como ID\n","\n","# Sample -> Como ID podemos tener un UUID o utilizar el Sample ID que viene en el pdf\n","# Otra vez, el valor del PDF no el global por lo que deberiamos utilizar un ID compuesto\n","# Por otro lado el ensayo anterior utiliza UUIDs para eso, lo que me parece mejor\n","# From y To salen de los datos\n","\n","# LabTest -> LabTestID (SRK-123-1234), el resto de los datos no estan en el informe\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"c7eb8af5-3089-4721-b463-0e678b71a636","normalized_state":"finished","queued_time":"2025-05-31T00:59:25.2498146Z","session_start_time":null,"execution_start_time":"2025-05-31T00:59:25.251122Z","execution_finish_time":"2025-05-31T00:59:26.3334601Z","parent_msg_id":"bd8fd554-cfa7-44b9-8675-e0ee8b08eaee"},"text/plain":"StatementMeta(, c7eb8af5-3089-4721-b463-0e678b71a636, 8, Finished, Available, Finished)"},"metadata":{}}],"execution_count":7,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cd935e70-ca19-4345-9852-eff666e530e5"},{"cell_type":"code","source":["##\n","# Construye las tablas si es que no existen - VERSIÓN CORREGIDA\n","##\n","from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n","\n","# PSD\n","psd_schema = StructType([\n","    StructField(\"LabTestID\", StringType(), nullable=False),\n","    StructField(\"PSDType\", StringType(), nullable=True),\n","    StructField(\"SieveSize_mm\", StringType(), nullable=False),\n","    StructField(\"PercentPassing\", DoubleType(), nullable=True),  # CAMBIO: DoubleType y nullable=True\n","    StructField(\"createdAt\", TimestampType(), nullable=False)\n","])\n","\n","# Atterberg\n","atterberg_schema = StructType([\n","    StructField(\"LabTestID\", StringType(), nullable=False),\n","    StructField(\"LiquidLimit\", DoubleType(), nullable=True),  # CAMBIO: DoubleType\n","    StructField(\"PlasticLimit\", DoubleType(), nullable=True),  # CAMBIO: DoubleType\n","    StructField(\"PlasticityIndex\", DoubleType(), nullable=True),  # CAMBIO: DoubleType\n","    StructField(\"ContractionPerc\", DoubleType(), nullable=True),  # CAMBIO: DoubleType\n","    StructField(\"Classification\", StringType(), nullable=True),\n","    StructField(\"createdAt\", TimestampType(), nullable=False)\n","])\n","\n","# GrainSize\n","grain_size_schema = StructType([\n","    StructField(\"LabTestID\", StringType(), nullable=False),\n","    StructField(\"CobblePercent\", DoubleType(), nullable=True),  # CAMBIO: DoubleType\n","    StructField(\"GravelPercent\", DoubleType(), nullable=True),  # CAMBIO: DoubleType\n","    StructField(\"SandPercent\", DoubleType(), nullable=True),  # CAMBIO: DoubleType\n","    StructField(\"FinePercent\", DoubleType(), nullable=True),  # CAMBIO: DoubleType\n","    StructField(\"SiltPercent\", DoubleType(), nullable=True),  # CAMBIO: DoubleType\n","    StructField(\"ClayPercent\", DoubleType(), nullable=True),  # CAMBIO: DoubleType\n","    StructField(\"createdAt\", TimestampType(), nullable=False)\n","])\n","\n","# Crea las tablas si es que no existen ya\n","spark.createDataFrame([], psd_schema).write.format(\"delta\").mode(\"ignore\").saveAsTable(\"PSD\")\n","spark.createDataFrame([], atterberg_schema).write.format(\"delta\").mode(\"ignore\").saveAsTable(\"Atterberg\")\n","spark.createDataFrame([], grain_size_schema).write.format(\"delta\").mode(\"ignore\").saveAsTable(\"GrainSize\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"c7eb8af5-3089-4721-b463-0e678b71a636","normalized_state":"finished","queued_time":"2025-05-31T00:59:25.5936981Z","session_start_time":null,"execution_start_time":"2025-05-31T00:59:26.3355413Z","execution_finish_time":"2025-05-31T00:59:27.8139448Z","parent_msg_id":"2456e3d8-d6aa-4527-9e38-22220b7d83ae"},"text/plain":"StatementMeta(, c7eb8af5-3089-4721-b463-0e678b71a636, 9, Finished, Available, Finished)"},"metadata":{}}],"execution_count":8,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"109327d8-f269-4af5-88d8-cbc6ae48da29"},{"cell_type":"code","source":["##\n","# Leemos los datos que se van a procesar - VERSIÓN CORREGIDA\n","##\n","import pandas as pd\n","import numpy as np\n","from uuid import uuid4\n","from datetime import datetime\n","\n","# Se leen desde la tabla de bronce todos los datos que corresponden a esta fuente de datos\n","PSD_SOURCE_NAME = \"SRK-169-Summary Sheet.pdf\"\n","\n","psd_df = spark.read.table(\"bronze_shortcut.data_PSD\").where(f\"Data_filename = '{PSD_SOURCE_NAME}'\").toPandas()\n","atter_df = spark.read.table(\"bronze_shortcut.data_atterberg\").where(f\"Data_filename = '{PSD_SOURCE_NAME}'\").toPandas()\n","\n","# Función para limpiar valores numéricos\n","def clean_numeric_value(value):\n","    \"\"\"Convierte valores a numérico manejando casos especiales\"\"\"\n","    if pd.isna(value) or value in ['-', 'NP', '']:\n","        return np.nan\n","    try:\n","        return float(value)\n","    except (ValueError, TypeError):\n","        return np.nan\n","\n","def build_psd_records(full_psd, row_i):\n","    \"\"\"Toma una de las filas de la tabla de bronce y la convierte \n","    en un conjunto de valores de la tabla de plata\"\"\"\n","    # Solo queremos los valores\n","    sieve_sizes = list(set(full_psd.columns) - set(['Data_filename', 'Depth_m', 'GM', 'Lab_No', 'Sample']))\n","    df = full_psd.loc[:, sieve_sizes].iloc[row_i]\n","\n","    # Transponemos y movemos el indice a una columna\n","    df = df.T.reset_index()\n","\n","    # Renombramos las columnas para que matcheen con la tabla de PSD\n","    df = df.rename(columns={\"index\": \"SieveSize_mm\", row_i: \"PercentPassing\"})\n","    \n","    # CAMBIO: Limpieza de valores numéricos\n","    df[\"PercentPassing\"] = df[\"PercentPassing\"].apply(clean_numeric_value)\n","\n","    # Agregamos los valores que faltan\n","    df[\"LabTestID\"] = full_psd.iloc[row_i][\"Lab_No\"]\n","    df[\"PSDType\"] = \"\"\n","    df[\"createdAt\"] = datetime.now()\n","\n","    return df\n","\n","def build_PSD_table(df):\n","    \"\"\"Convierte todos los datos de PSD en registros de la tabla de plata\"\"\"\n","    all_psd = [\n","        build_psd_records(df, i)\n","        for i in range(len(df))\n","    ]\n","    \n","    return pd.concat(all_psd, ignore_index=True)\n","\n","def build_atterberg_table(df):\n","    \"\"\"Convierte todos los datos de Atterberg en registros de la tabla de plata\"\"\"\n","    # Solo las columnas que nos interesan\n","    att = df.loc[:, [\"Lab_No\", \"LL_%\", \"PI_%\", \"LS_%\"]].copy()\n","\n","    # CAMBIO: Limpieza de valores numéricos\n","    for col in ['LL_%', 'PI_%', 'LS_%']:\n","        att[col] = att[col].apply(clean_numeric_value)\n","\n","    # Son renombradas\n","    att = att.rename(columns={\n","        \"Lab_No\": \"LabTestID\", \n","        \"LL_%\": \"LiquidLimit\", \n","        \"PI_%\": \"PlasticLimit\", \n","        \"LS_%\": \"ContractionPerc\"\n","    })\n","\n","    # Agregamos las que faltan\n","    att[\"PlasticityIndex\"] = 0\n","    att[\"Classification\"] = \"\"\n","    att[\"createdAt\"] = datetime.now()\n","    \n","    return att\n","\n","def _calc_depths(depths):\n","    \"\"\"Calcula las profundidades \"From\", \"Middle\" y \"To\", las devuelve en tuplas\"\"\"\n","    # Si solo tengo una profundidad; son iguales\n","    if \"-\" not in depths:\n","        try:\n","            depth = float(depths.strip())\n","            return depth, depth, depth\n","        except:\n","            return np.nan, np.nan, np.nan\n","    \n","    # Tengo dos profundidades, calculo la media\n","    try:\n","        profundidades = [float(d.strip()) for d in depths.split(\"-\")]\n","        prof_media = sum(profundidades)/2\n","        return profundidades[0], prof_media, profundidades[1]\n","    except:\n","        return np.nan, np.nan, np.nan\n","\n","def build_tables(df):\n","    \"\"\"Construye las tablas \"LabTest\" y \"Sample\" para la capa de plata\"\"\"\n","    # Solo nos interesan ciertas columnas\n","    df = df.loc[:, [\"Lab_No\", \"Sample\", \"Depth_m\"]].rename(columns={\"Lab_No\": \"LabTestID\", \"Sample\": \"Comment\"}).copy()\n","\n","    # Calculamos los valores que nos faltan\n","    df[\"SampleID\"] = df.apply(lambda r: str(uuid4()), axis=1)\n","    df[\"uuid\"] = df.apply(lambda r: str(uuid4()), axis=1)\n","    \n","    # CAMBIO: Manejo de errores en profundidades\n","    depths = df[\"Depth_m\"].apply(_calc_depths)\n","    df[\"DepthFrom_m\"] = depths.apply(lambda x: x[0])\n","    df[\"MiddleDepth_m\"] = depths.apply(lambda x: x[1])\n","    df[\"DepthTo_m\"] = depths.apply(lambda x: x[2])\n","\n","    # Some empty but mandatory columns\n","    df[\"TestType\"] = \"\"\n","    df[\"TestDate\"] = None\n","    df[\"ReceivedDate\"] = None\n","    df[\"LocationID\"] = None\n","    df[\"SampleType\"] = None\n","    df[\"MaterialType\"] = None\n","    df[\"State\"] = None\n","    df[\"Laboratory\"] = None\n","    df[\"createdAt\"] = datetime.now()\n","\n","    labtest = df.loc[:, [\n","        \"uuid\", \"LabTestID\", \"SampleID\", \"TestType\", \"Comment\", \n","        \"TestDate\", \"ReceivedDate\", \"createdAt\"\n","    ]]\n","    sample =  df.loc[:, [\n","        \"SampleID\", \"DepthFrom_m\", \"MiddleDepth_m\", \"DepthTo_m\", \n","        \"Comment\", \"LocationID\", \"SampleType\", \"MaterialType\", \n","        \"State\", \"Laboratory\", \"createdAt\"\n","    ]]\n","\n","    return labtest, sample\n","\n","PSD_table_data = build_PSD_table(psd_df)\n","Atterberg_table_data = build_atterberg_table(atter_df)\n","\n","def verificar_esquema(df, tabla):\n","    df_columns = set(df.columns)\n","    tabla_columns = set(spark.table(tabla).columns)\n","    \n","    print(f\"\\nVerificación para {tabla}:\")\n","    print(f\"Columnas en DF faltantes en tabla: {df_columns - tabla_columns}\")\n","    print(f\"Columnas en tabla faltantes en DF: {tabla_columns - df_columns}\")\n","    \n","    return df_columns == tabla_columns\n","\n","verificar_esquema(Atterberg_table_data, \"Atterberg\")\n","verificar_esquema(PSD_table_data, \"PSD\")\n","\n","labtest_table, sample_table = build_tables(psd_df)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":10,"statement_ids":[10],"state":"finished","livy_statement_state":"available","session_id":"c7eb8af5-3089-4721-b463-0e678b71a636","normalized_state":"finished","queued_time":"2025-05-31T00:59:26.2946792Z","session_start_time":null,"execution_start_time":"2025-05-31T00:59:27.8162998Z","execution_finish_time":"2025-05-31T00:59:29.2674872Z","parent_msg_id":"5f3be901-0dfc-46d7-8bdd-c6fbeaf2920a"},"text/plain":"StatementMeta(, c7eb8af5-3089-4721-b463-0e678b71a636, 10, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\nVerificación para Atterberg:\nColumnas en DF faltantes en tabla: {'ContractionPerc'}\nColumnas en tabla faltantes en DF: {'uuid'}\n\nVerificación para PSD:\nColumnas en DF faltantes en tabla: set()\nColumnas en tabla faltantes en DF: {'uuid'}\n"]}],"execution_count":9,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ea8b8f56-3fb8-488a-b85c-50e30bbe12ee"},{"cell_type":"code","source":["##\n","# Guardamos los datos en las tablas de plata\n","##\n","from delta.tables import DeltaTable\n","from pyspark.sql.functions import expr\n","from uuid import uuid4\n","\n","def generate_uuid_with_dashes():\n","    \"\"\"Función para generar UUIDs con guiones\"\"\"\n","    return str(uuid4())\n","\n","# Registramos la UDF\n","spark.udf.register(\"generate_uuid_with_dashes\", generate_uuid_with_dashes)\n","\n","def guardar_datos(datos, nombre_tabla):\n","    \"\"\"Guarda los datos en la tabla dada.\"\"\"\n","    nuevos_datos = spark.createDataFrame(datos)\n","    \n","    if \"uuid\" not in nuevos_datos.columns:\n","        nuevos_datos = nuevos_datos.withColumn(\"uuid\", expr(\"generate_uuid_with_dashes()\"))\n","    \n","    columnas = nuevos_datos.columns\n","\n","    # Condición de merge dinámica\n","    merge_condition = (\"old.LabTestID = new.LabTestID\" if nombre_tabla in [\"PSD\", \"Atterberg\", \"LabTest\"] \n","                      else \"old.SampleID = new.SampleID\")\n","\n","    DeltaTable.forName(spark, nombre_tabla).alias(\"old\").merge(\n","        nuevos_datos.alias(\"new\"),\n","        merge_condition\n","    ).whenNotMatchedInsertAll().execute()\n","\n","# Procesamiento\n","guardar_datos(PSD_table_data, \"PSD\")\n","guardar_datos(Atterberg_table_data, \"Atterberg\")\n","guardar_datos(labtest_table, \"LabTest\")\n","guardar_datos(sample_table, \"Sample\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":11,"statement_ids":[11],"state":"finished","livy_statement_state":"available","session_id":"c7eb8af5-3089-4721-b463-0e678b71a636","normalized_state":"finished","queued_time":"2025-05-31T00:59:27.4625823Z","session_start_time":null,"execution_start_time":"2025-05-31T00:59:29.2697259Z","execution_finish_time":"2025-05-31T00:59:38.0480518Z","parent_msg_id":"3016e276-5e20-4490-9fa4-f7f315391ef4"},"text/plain":"StatementMeta(, c7eb8af5-3089-4721-b463-0e678b71a636, 11, Finished, Available, Finished)"},"metadata":{}}],"execution_count":10,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"579ee262-44b7-4a75-8f25-cb02c0d806ae"},{"cell_type":"code","source":["%%sql\n","select * from psd limit 5"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":15,"statement_ids":[15],"state":"finished","livy_statement_state":"available","session_id":"c7eb8af5-3089-4721-b463-0e678b71a636","normalized_state":"finished","queued_time":"2025-05-31T01:06:18.1211126Z","session_start_time":null,"execution_start_time":"2025-05-31T01:06:18.1223388Z","execution_finish_time":"2025-05-31T01:06:20.4281282Z","parent_msg_id":"2d387b40-f472-4621-8626-3a56c3c56669"},"text/plain":"StatementMeta(, c7eb8af5-3089-4721-b463-0e678b71a636, 15, Finished, Available, Finished)"},"metadata":{}},{"output_type":"execute_result","execution_count":14,"data":{"application/vnd.synapse.sparksql-result+json":{"schema":{"type":"struct","fields":[{"name":"uuid","type":"string","nullable":false,"metadata":{}},{"name":"LabTestID","type":"string","nullable":false,"metadata":{}},{"name":"PSDType","type":"string","nullable":true,"metadata":{}},{"name":"SieveSize_mm","type":"string","nullable":true,"metadata":{}},{"name":"PercentPassing","type":"string","nullable":true,"metadata":{}},{"name":"createdAt","type":"timestamp","nullable":false,"metadata":{}}]},"data":[["d08814d5-69b0-4d2f-9fb6-182c41181754","SRK-169-3094","","0.250","98","2025-05-30T23:21:16Z"],["f5e17192-39c3-4217-abe6-0e986114f0b1","SRK-169-3094","","0.035","57","2025-05-30T23:21:16Z"],["f0c20a98-f8e1-4162-afca-76597d13bdef","SRK-169-3094","","0.075","83","2025-05-30T23:21:16Z"],["3d930f8e-aff6-4c75-b227-429f15da8f70","SRK-169-3094","","0.060","76","2025-05-30T23:21:16Z"],["970e90ca-490a-4462-9bf0-d2ec32014a5d","SRK-169-3094","","0.002","10","2025-05-30T23:21:16Z"]]},"text/plain":"<Spark SQL result set with 5 rows and 6 fields>"},"metadata":{}}],"execution_count":14,"metadata":{"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"collapsed":false},"id":"263c6fe5-93c8-48ce-b756-f4e338e8c847"},{"cell_type":"code","source":[],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"05944090-e243-4b67-81d4-08696b21a95b"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"c14a86b2-f247-44a5-8d06-0dafd853babf"}],"default_lakehouse":"c14a86b2-f247-44a5-8d06-0dafd853babf","default_lakehouse_name":"silver","default_lakehouse_workspace_id":"cd0fc9d8-dd24-4361-bd63-c66c06cf2316"}}},"nbformat":4,"nbformat_minor":5}